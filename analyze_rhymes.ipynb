{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal is to detect three types of rhyming techniques: \n",
    "- assonance.\n",
    "- mutli-syllable rhymes.\n",
    "- rhyming schemes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read data: \n",
    "Read the lyrics in a list of lines. \n",
    "- preprocessing steps: \n",
    "1. remove special characters. \n",
    "2. remove maningless words? (implement it but keep it open). \n",
    "3. create a vowel representation for each word (assonance detection)\n",
    "4. create a syllable representation of each word (multis detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pronouncing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'come']\n"
     ]
    }
   ],
   "source": [
    "import pyphen\n",
    "\n",
    "# Create an instance of the Pyphen class using the 'en' dictionary for English language\n",
    "dic = pyphen.Pyphen(lang='en_US')\n",
    "\n",
    "# Get the syllables of a word\n",
    "word = 'income'\n",
    "syllables = dic.inserted(word).split('-')\n",
    "\n",
    "print(syllables)  # Output: ['ex', 'am', 'ple']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read & Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zohabidi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "new_stop_words = ['ooh','yeah','hey','whoa','woah', 'ohh', 'was', 'mmm', 'oooh','yah','yeh','mmm', 'hmm','deh','doh','jah','wa']\n",
    "stop_words.extend(new_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "lines,words = [], []\n",
    "punctuation = string.punctuation.replace(\"'\", \"\")\n",
    "translation_table = str.maketrans(punctuation, ' ' * len(punctuation))\n",
    "\n",
    "def remove_extra_spaces(input_string):\n",
    "    # Split the string into words and join them with a single space\n",
    "    return ' '.join(input_string.split())\n",
    "\n",
    "with open(\"lyrics_en\\\\2pac\\\\Hit_'Em_Up.txt\", 'r') as file:\n",
    "    # Iterate over each line in the file\n",
    "    for line in file:\n",
    "        # Strip leading/trailing whitespace from the line\n",
    "        line = line.replace(\"\\n\", \" \")\n",
    "        # Replace punctuation marks with spaces\n",
    "        line = line.translate(translation_table)\n",
    "        line = remove_extra_spaces(line)\n",
    "        ## remove stopwords not so sure about it. \n",
    "        \n",
    "        lines.append(line)\n",
    "        words.append(line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"ain't\", 'got', 'no', \"motherfuckin'\", 'friends', 'sucka', 'ass']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'M0RFKN'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create vowel representation: \n",
    "import phonetics\n",
    "#ph.get_phonetic_transcription('bath')\n",
    "phonetics.metaphone('motherfuckin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check rhyming words\n",
    "phonetics.metaphone('friends')[-1] == phonetics.metaphone('ass')[-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar1 = \"sling some raps\"\n",
    "bar2 = \"income tax\"\n",
    "\n",
    "words_bar1 = bar1.split(\" \")\n",
    "words_bar2 = bar2.split(\" \")\n",
    "\n",
    "bar1_syllables,bar2_syllables = [],[]\n",
    "bar1_ph_syllables,bar2_ph_syllables = [],[]\n",
    "\n",
    "\n",
    "for word in words_bar1:\n",
    "    syllables = dic.inserted(word).split('-')\n",
    "    bar1_syllables.append(syllables)\n",
    "    syllables_ph = []\n",
    "    for syllable in syllables:\n",
    "        syllables_ph.append(phonetics.nysiis(syllable))\n",
    "    bar1_ph_syllables.append(syllables_ph)\n",
    "\n",
    "for word in words_bar2:\n",
    "    syllables = dic.inserted(word).split('-')\n",
    "    bar2_syllables.append(syllables)\n",
    "    syllables_ph = []\n",
    "    for syllable in syllables:\n",
    "        syllables_ph.append(phonetics.nysiis(syllable))\n",
    "    bar2_ph_syllables.append(syllables_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['SA'], ['SANA'], ['RA']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bar1_ph_syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['IA', 'CANA'], ['TA']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bar2_ph_syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar1 = \"Cold Winter Day so I wear my jacket\" \n",
    "bar2 = \"gold winners play because life is a game\"\n",
    "\n",
    "words_bar1 = bar1.split(\" \")\n",
    "words_bar2 = bar2.split(\" \")\n",
    "\n",
    "bar1_syllables,bar2_syllables = [],[]\n",
    "bar1_ph_syllables,bar2_ph_syllables = [],[]\n",
    "\n",
    "\n",
    "for word in words_bar1:\n",
    "    syllables = dic.inserted(word).split('-')\n",
    "    bar1_syllables.append(syllables)\n",
    "    syllables_ph = []\n",
    "    for syllable in syllables:\n",
    "        syllables_ph.append(phonetics.metaphone(syllable))\n",
    "    bar1_ph_syllables.append(syllables_ph)\n",
    "\n",
    "for word in words_bar2:\n",
    "    syllables = dic.inserted(word).split('-')\n",
    "    bar2_syllables.append(syllables)\n",
    "    syllables_ph = []\n",
    "    for syllable in syllables:\n",
    "        syllables_ph.append(phonetics.metaphone(syllable))\n",
    "    bar2_ph_syllables.append(syllables_ph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['KLT'], ['AN', 'TR'], ['T'], ['S'], ['A'], ['AR'], ['M'], ['JK', 'AT']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bar1_ph_syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['KLT'], ['AN', 'NRS'], ['PL'], ['P', 'KS'], ['LF'], ['AS'], ['A'], ['KM']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bar2_ph_syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein similarity: 0.57\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def levenshtein_similarity(str1, str2):\n",
    "    # Calculate the Levenshtein distance\n",
    "    distance = Levenshtein.distance(str1, str2)\n",
    "    # Calculate the similarity ratio\n",
    "    similarity = 1 - (distance / max(len(str1), len(str2)))\n",
    "    return similarity\n",
    "\n",
    "# Example usage\n",
    "str1 = \"KLT AN TR T S A AR\"\n",
    "str2 = \"KLT AN NRS PL P KS LF\"\n",
    "similarity = levenshtein_similarity(str1, str2)\n",
    "print(f\"Levenshtein similarity: {similarity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def difflib_similarity(str1, str2):\n",
    "    # Create a SequenceMatcher object\n",
    "    matcher = SequenceMatcher(None, str1, str2)\n",
    "    # Calculate the similarity ratio\n",
    "    similarity = matcher.ratio()\n",
    "    return similarity\n",
    "\n",
    "#similarity = difflib_similarity(str1, str2)\n",
    "#print(f\"difflib similarity: {similarity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract multi syllable rhymes --> assonance extraction --> rhyme schemes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['B', 'C'], ['B', 'C'])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_threshold = 0.75\n",
    "def detect_multi_rhymes(current_line_syllables, next_line_syllables):\n",
    "    ## for each syllable in the current line check current line and next line. \n",
    "    # if you find matching syllables, start concatenating the matching syllable strings, stop when the similarity is under a specific threshold\n",
    "    ## Sample input\n",
    "    ## current_line_syllables = [\"A\", \"B\", \"C\"]\n",
    "    ## next_line_syllables = [\"D\", \"B\", \"C\"]\n",
    "    i=0\n",
    "    mutli_rhymes = []\n",
    "\n",
    "    while(i<(len(current_line_syllables)-1)):\n",
    "\n",
    "        j,extension = 0, 0\n",
    "        lookup_field = current_line_syllables[i+1:] + next_line_syllables\n",
    "\n",
    "        while((i+extension)<len(current_line_syllables) and (j+extension) < len(lookup_field)): \n",
    "            str1 = ' '.join(current_line_syllables[i:i+1+extension])\n",
    "            str2 = ' '.join(lookup_field[j:j+1+extension])\n",
    "\n",
    "            similarity = difflib_similarity(str1, str2)\n",
    "            if similarity >= similarity_threshold:\n",
    "                extension +=1\n",
    "            elif similarity<similarity_threshold and extension >1:\n",
    "                break\n",
    "            elif similarity<similarity_threshold and extension <=1:\n",
    "                j+=1\n",
    "                extension = 0\n",
    "        \n",
    "        if extension >1:\n",
    "            mutli_rhyme = (current_line_syllables[i:i+extension],lookup_field[j:j+extension])   \n",
    "            mutli_rhymes.append(mutli_rhyme)\n",
    "        \n",
    "        i = i+extension+1\n",
    "    \n",
    "    return mutli_rhymes\n",
    "\n",
    "detect_multi_rhymes([\"A\", \"B\", \"C\"], [\"D\", \"B\", \"C\"])\n",
    "#detect_multi_rhymes([\"A\", \"B\", \"C\",\"D\", \"M\", \"N\", \"O\"], [\"E\",\"F\", \"B\", \"L\",\"B\", \"C\", \"D\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KM'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonetics.metaphone(\"com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['sling', 'some', 'raps'], ['SLNK', 'SM', 'RPS'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_phonetic_syllable_represtation(line):\n",
    "    words_list = line.split(\" \")\n",
    "    line_syllables,line_syllables_ph = [],[]\n",
    "\n",
    "    for word in words_list:\n",
    "        syllables = dic.inserted(word).split('-')\n",
    "        line_syllables.extend(syllables)\n",
    "        for syllable in syllables:\n",
    "            line_syllables_ph.append(phonetics.metaphone(syllable))\n",
    "    return line_syllables,line_syllables_ph\n",
    "\n",
    "create_phonetic_syllable_represtation(\"sling some raps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each line -> create syllable phonetic representation of current & next line --> call the multi function\n",
    "# -> in case you find something add it to a dict: key:line number -> value: multi tuple. \n",
    "multi_rhymes = {}\n",
    "for line_index in range(len(lines)-1):\n",
    "    _,current_line_syllables_ph=create_phonetic_syllable_represtation(lines[line_index])\n",
    "    _,next_line_syllables_ph=create_phonetic_syllable_represtation(lines[line_index+1])\n",
    "    \n",
    "    multi_rhyme = detect_multi_rhymes(current_line_syllables_ph, next_line_syllables_ph)\n",
    "    if multi_rhyme is not None:\n",
    "        multi_rhymes[line_index] = multi_rhyme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 2.0\n"
     ]
    }
   ],
   "source": [
    "number_multi_rhymes, average_len_multi_rhymes = 0,0\n",
    "for index, (key,value) in enumerate(multi_rhymes.items()):\n",
    "    if len(value)>0:\n",
    "        number_multi_rhymes += 1\n",
    "        average_len_multi_rhymes += len(value[0])\n",
    "\n",
    "print(number_multi_rhymes, average_len_multi_rhymes/number_multi_rhymes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assonance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same logic, you check current line and next line \n",
    "# you need a vowel representation \n",
    "def cut_at_last_vowel(line):\n",
    "    vowels = \"aeiou\"\n",
    "    words = line.split(\" \")\n",
    "    line_vowel_repr = []\n",
    "\n",
    "    for word in words:\n",
    "        last_vowel_pos = -1\n",
    "\n",
    "        # Find the position of the last vowel\n",
    "        for i, char in enumerate(reversed(word.lower())):\n",
    "            if char in vowels:\n",
    "                last_vowel_pos = len(word) - 1 - i\n",
    "                break\n",
    "    \n",
    "        # If no vowel is found, return the original string\n",
    "        if last_vowel_pos != -1:\n",
    "            line_vowel_repr.append(word[:last_vowel_pos + 1].lower()) \n",
    "\n",
    "    return line_vowel_repr\n",
    "\n",
    "def delete_indices_from_list(input_list, indices_to_delete):\n",
    "    # Sort the indices in reverse order\n",
    "    indices_to_delete = sorted(indices_to_delete, reverse=True)\n",
    "    \n",
    "    # Delete elements at the specified indices\n",
    "    for index in indices_to_delete:\n",
    "        del input_list[index]\n",
    "    \n",
    "    return input_list\n",
    "\n",
    "def detect_assonance(current_line_vowel_repr, next_line_vowel_repr):\n",
    "    assonances = []\n",
    "    i = 0\n",
    "    while(i<len(current_line_vowel_repr)):\n",
    "        j = i+1\n",
    "        assonance = []\n",
    "        indices_to_delete_current_line = []\n",
    "        while (j<len(current_line_vowel_repr)):\n",
    "            if current_line_vowel_repr[i][-1] == current_line_vowel_repr[j][-1]: \n",
    "                assonance.append(current_line_vowel_repr[j])\n",
    "                indices_to_delete_current_line.append(j)\n",
    "            j+=1\n",
    "\n",
    "        j = 0\n",
    "        while (j<len(next_line_vowel_repr)):\n",
    "            if current_line_vowel_repr[i][-1] == next_line_vowel_repr[j][-1]: \n",
    "                assonance.append(next_line_vowel_repr[j])\n",
    "            j+=1\n",
    "\n",
    "        if len(assonance):\n",
    "            assonance.insert(0,current_line_vowel_repr[i])\n",
    "            assonances.append(list(set(assonance)))\n",
    "            indices_to_delete_current_line.append(i)\n",
    "            delete_indices_from_list(current_line_vowel_repr, indices_to_delete_current_line)\n",
    "        else:\n",
    "            i+=1\n",
    "    \n",
    "    return assonances\n",
    "\n",
    "#detect_assonance([ \"BA\", \"MAI\", \"LOO\", \"FA\"], [\"TAA\", \"LBO\", \"KKO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "assonances = {}\n",
    "for line_index in range(len(lines)-1):\n",
    "    current_line_vowel_repr = cut_at_last_vowel(lines[line_index])\n",
    "    if not len(current_line_vowel_repr):\n",
    "        continue\n",
    "    next_line_vowel_repr = cut_at_last_vowel(lines[line_index+1])\n",
    "    \n",
    "    assonance = detect_assonance(current_line_vowel_repr, next_line_vowel_repr)\n",
    "    if len(assonance):\n",
    "        assonances[line_index] = assonance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [['i', 'ai', 'motherfucki', 'bi'],\n",
       "  ['go', 'no', 'yo'],\n",
       "  ['fucke', 'motherfucke', 'frie'],\n",
       "  ['tha', 'sucka', 'a', 'fa']],\n",
       " 1: [['ba', 'fa', 'tha'],\n",
       "  ['i', 'bi'],\n",
       "  ['motherfucke', 'fucke', 'we', 'mone', 'take', 'side', 'kille'],\n",
       "  ['yo', 'bo']],\n",
       " 2: [['we', 'mone', 'the', 'take', 'reale', 'side', 'kille'],\n",
       "  ['ba', 'nigga'],\n",
       "  ['kno', 'who', 'to', 'bo']],\n",
       " 3: [['you'],\n",
       "  ['kno', 'who', 'to'],\n",
       "  ['we', 'mone', 'the', 'take', 'reale'],\n",
       "  ['i', 'ai', 'bri'],\n",
       "  ['tha', 'nigga']],\n",
       " 5: [['the', 'clique', 'take', 'mone']],\n",
       " 6: [['fi', 'wi', 'clai', 'bi'],\n",
       "  ['fu', 'you'],\n",
       "  ['westside',\n",
       "   'we',\n",
       "   'ride',\n",
       "   'equippe',\n",
       "   'the',\n",
       "   'clique',\n",
       "   'come',\n",
       "   'game',\n",
       "   'whe']],\n",
       " 7: [['westside',\n",
       "   'fucke',\n",
       "   'playe',\n",
       "   'we',\n",
       "   'ride',\n",
       "   'equippe',\n",
       "   'be',\n",
       "   'come',\n",
       "   'game',\n",
       "   'wife',\n",
       "   'whe'],\n",
       "  ['wi', 'i', 'clai']],\n",
       " 8: [['bu', 'you'],\n",
       "  ['clai', 'i'],\n",
       "  ['fo', 'o', 'to', 'bo'],\n",
       "  ['fucke', 'life', 'playe', 'we', 'be', 'wife'],\n",
       "  ['ba', 'a', 'nigga']],\n",
       " 9: [['see', 'fucke', 'life', 'we', 'me'],\n",
       "  ['plu', 'pu', 'bu'],\n",
       "  ['fo', 'o', 'to', 'bo'],\n",
       "  ['ba', 'hea', 'nigga', 'wea']],\n",
       " 10: [['pu', 'plu'],\n",
       "  ['tryi', 'i', 'ri'],\n",
       "  ['junio', 'to'],\n",
       "  ['see', 'biggie', 'some', 'bitche', 'me'],\n",
       "  ['a', 'ma', 'hea', 'sma', 'wea']],\n",
       " 11: [['while', 'biggie', 'some', 'we', 'jewe', 'kee', 'bitche'],\n",
       "  ['ma', 'sma', 'a'],\n",
       "  ['yo', 'junio', 'o', 'fo'],\n",
       "  ['i', 'runni', 'comi']],\n",
       " 12: [['while', 'rule', 'we', 'jewe', 'the', 'kee'],\n",
       "  ['o', 'foo', 'fo', 'kno', 'yo'],\n",
       "  ['gunni', 'runni', 'busti', 'comi']],\n",
       " 13: [['stea', 'ya', 'a'],\n",
       "  ['i', 'gunni', 'li', 'busti'],\n",
       "  ['rule', 'homie', 'kee', 'the', 'leave', 'cease'],\n",
       "  ['o', 'ho', 'foo', 'go', 'kno']],\n",
       " 14: [['i', 'li'],\n",
       "  ['piece', 'decease', 'homie', 'be', 'leave', 'cease'],\n",
       "  ['go', 'no', 'ho'],\n",
       "  ['ya', 'a']],\n",
       " 15: [['arou', 'cu', 'you', 'fu', 'u'],\n",
       "  ['rea', 'a'],\n",
       "  ['piece', 'be', 'leave', 'decease'],\n",
       "  ['wi', 'i', 'li', 'ki'],\n",
       "  ['no', 'do']],\n",
       " 16: [['wi', 'li', 'ki', 'qui'],\n",
       "  ['o', 'so', 'do', 'yo', 'to'],\n",
       "  ['fu', 'u', 'arou'],\n",
       "  ['rea', 'a', 'sna']],\n",
       " 17: [['i', 'qui'],\n",
       "  ['o', 'so', 'fo', 'kno', 'yo', 'to'],\n",
       "  ['sna', 'a', 'nigga'],\n",
       "  ['fu', 'u'],\n",
       "  ['le', 'life', 'the', 'stree', 'peace']],\n",
       " 18: [['i', 'toni'],\n",
       "  ['westside', 'le', 'life', 'ride', 'the'],\n",
       "  ['nigga', 'haha'],\n",
       "  ['do', 'kno', 'o', 'fo']],\n",
       " 19: [['bo', 'o', 'do'],\n",
       "  ['murdere', 'westside', 'le', 'ride', 'the', 'kille'],\n",
       "  ['ba', 'wa', 'a', 'haha']],\n",
       " 20: [['ca', 'ba', 'wa', 'a'],\n",
       "  ['kno', 'yo', 'o', 'bo'],\n",
       "  ['murdere', 'ge', 'see', 'peele', 'me', 'kille']],\n",
       " 21: [['fu', 'you'],\n",
       "  ['see', 'ge', 'peele', 'me', 'whe'],\n",
       "  ['ca', 'a', 'ya', 'gra', 'tupa'],\n",
       "  ['kno', 'yo', 'glo']],\n",
       " 22: [['tupa', 'ya', 'ca', 'gra'],\n",
       "  ['co', 'glo'],\n",
       "  ['see', 'the', 'whe'],\n",
       "  ['you', 'u']],\n",
       " 23: [['ca', 'ya', 'tupa'],\n",
       "  ['see', 'the', 'whe', 'me'],\n",
       "  ['sho', 'who', 'co'],\n",
       "  ['pu', 'bu', 'you', 'u']],\n",
       " 24: [['who', 'o', 'sho', 'no', 'to'],\n",
       "  ['me', 'menace', 'the', 'fee'],\n",
       "  ['pu', 'bu', \"'bou\"],\n",
       "  ['wra', 'a', 'ya'],\n",
       "  ['di', 'fini']],\n",
       " 25: [['no', 'o', 'to'],\n",
       "  ['wra', 'a', 'ya', 'nigga'],\n",
       "  [\"'bou\", 'u'],\n",
       "  ['the', 'menace', 'e', 'fee']],\n",
       " 26: [['wha', 'nigga'],\n",
       "  ['i', 'hi', 'thi'],\n",
       "  ['motherfucke', 'che', 'mone', 'take', 'e', 'time'],\n",
       "  ['ou', 'you', 'u']],\n",
       " 27: [['motherfucke', 'che', 'mone', 'take', 'eve', 'time'],\n",
       "  ['i', 'thi'],\n",
       "  ['ou', 'you'],\n",
       "  ['kno', 'o', 'do'],\n",
       "  ['wha', 'tra']],\n",
       " 28: [['i', 'ai', 'thi'],\n",
       "  ['kno', 'o', 'do'],\n",
       "  ['leve', 'eve', 'take', 'mone'],\n",
       "  ['nigga', 'tra', \"y'a\"]],\n",
       " 29: [[\"i'ma\", 'nigga', \"y'a\"],\n",
       "  ['le', 'little', 'ride', 'mone', 'homie', 'leve', 'take', 'eve'],\n",
       "  ['o']],\n",
       " 30: [[\"i'ma\", 'ba', 'a'],\n",
       "  ['made',\n",
       "   'fee',\n",
       "   'le',\n",
       "   'little',\n",
       "   'he',\n",
       "   'ride',\n",
       "   'mone',\n",
       "   'homie',\n",
       "   'the',\n",
       "   'bitche',\n",
       "   'take'],\n",
       "  ['yo', 'o', 'ho', 'bo'],\n",
       "  ['you', 'u', 'fu']],\n",
       " 31: [['i', 'bi'],\n",
       "  ['made', 'ge', 'fee', 'he', 'the', 'bitche'],\n",
       "  ['ba', 'wa', 'a'],\n",
       "  ['yo', 'ho', 'bo'],\n",
       "  ['ou', 'fu', 'u']],\n",
       " 32: [['ge', 'biggie', 'droppe', 'the'],\n",
       "  ['ju', 'ou'],\n",
       "  ['sma', 'wa'],\n",
       "  ['go', 'yo']],\n",
       " 33: [['biggie', 'droppe', 'le', 'little', 'the', 'me'],\n",
       "  ['a', 'pa', 'ma', 'sma', 'ba'],\n",
       "  ['ju', 'mu']],\n",
       " 34: [['ge', 'le', 'little', 'the', 'spanke', 'me', 'nee', 'white'],\n",
       "  ['a', 'pa', 'ma', 'fra', 'tra', 'ba'],\n",
       "  ['setti', 'hi', 'ri', 'i']],\n",
       " 35: [['a', 'hea', 'ya', 'fra', 'tra'],\n",
       "  ['murdere', 'ge', 'neve', 'little', 'accide', 'spanke', 'nee', 'white'],\n",
       "  ['o', 'to', 'fo'],\n",
       "  ['setti', 'i', 'ri', 'ai']],\n",
       " 36: [['murdere', 'neve', 'little', 'accide', 'whe'],\n",
       "  ['a', 'ga', 'hea', 'ya', 'atta'],\n",
       "  ['i', 'servi', 'ai']],\n",
       " 37: [['spa', 'ga', 'ya', 'atta', 'sha'],\n",
       "  ['whole', 'whe', 'style'],\n",
       "  ['i', 'servi']],\n",
       " 38: [['a', 'spa', 'ga', \"i'ma\", 'ya', 'sla', 'gua', 'sha', 'ra'],\n",
       "  [\"'cause\", 'whole', 'the', 'whe', 'style'],\n",
       "  ['i', 'pai']],\n",
       " 39: [['a', \"i'ma\", 'gua', 'tha', 'sla', 'nigga', 'ra'],\n",
       "  ['pu', 'you', 'throu'],\n",
       "  [\"'cause\", 'the', 'weake'],\n",
       "  ['i', 'runni', 'pai', 'fucki']],\n",
       " 40: [['pu', 'throu', 'you'],\n",
       "  ['weake', 'the'],\n",
       "  ['tha', 'a', 'nigga'],\n",
       "  ['smoki', 'i', 'runni', 'fucki'],\n",
       "  ['junio', 'fro', 'o', 'blo']],\n",
       " 41: [['rea', 'a', 'nigga'],\n",
       "  ['wi', 'i', 'smoki', 'tucki'],\n",
       "  ['junio', 'fro', 'o']],\n",
       " 42: [['wi', 'i', 'tucki', 'hi'],\n",
       "  ['unde',\n",
       "   \"'e\",\n",
       "   'baue',\n",
       "   'package',\n",
       "   'powe',\n",
       "   'the',\n",
       "   'pe',\n",
       "   'gue',\n",
       "   'eddie',\n",
       "   'eve'],\n",
       "  ['rea', 'ya']],\n",
       " 43: [['tupa', 'ya', 'gra'],\n",
       "  ['pu', 'clou', 'sou', 'you', 'u', 'hou'],\n",
       "  ['see', \"'e\", 'package', 'pe', 'whe', 'eve'],\n",
       "  ['i', 'hi']],\n",
       " 44: [['tupa', 'ya', 'ca', 'gra'],\n",
       "  ['co', 'glo'],\n",
       "  ['see', 'the', 'whe'],\n",
       "  ['you', 'u']],\n",
       " 45: [['ca', 'ya', 'tupa'],\n",
       "  ['see', 'the', 'whe', 'me'],\n",
       "  ['sho', 'who', 'co'],\n",
       "  ['pu', 'bu', 'you', 'u']],\n",
       " 46: [['who', 'o', 'sho', 'no', 'to'],\n",
       "  ['me', 'menace', 'the', 'fee'],\n",
       "  ['pu', 'bu', \"'bou\"],\n",
       "  ['wra', 'a', 'ya'],\n",
       "  ['di', 'fini']],\n",
       " 47: [['no', 'o', 'to'],\n",
       "  ['wra', 'a', 'ya', 'nigga'],\n",
       "  [\"'bou\", 'u'],\n",
       "  ['fee', 'we', 'the', 'menace', 'e']],\n",
       " 48: [['rea', 'penitentia', 'a', 'nigga'],\n",
       "  ['pee', 'we', 'stee', 'kee', 'e'],\n",
       "  ['hi', 'i']],\n",
       " 49: [['pee', 'freestyle', 'we', 'stee', 'kee', 'battle'],\n",
       "  ['no', 'ho', 'do'],\n",
       "  ['i', 'ai', 'thi'],\n",
       "  ['rea', 'penitentia', 'a']],\n",
       " 50: [['wi', 'getti', 'ai', 'thi'], ['freestyle', 'kille', 'battle', 'ope']],\n",
       " 51: [['a', 'tryna', 'ya', 'offa', 'nigga'],\n",
       "  ['clou', 'u', 'you', 'mou'],\n",
       "  ['wi', 'getti', 'hopi', 'i'],\n",
       "  ['ope', 'the', 'come', 'me', 'kille']],\n",
       " 52: [['tryna', 'offa', 'a', 'nigga'],\n",
       "  ['learne', 'the', 'dope', 'come', 'me', 'like', 'she'],\n",
       "  ['clou', 'you', 'u'],\n",
       "  ['hi', 'thi', 'i', 'smoki', 'hopi']],\n",
       " 53: [['i', 'smoki', 'hi', 'thi'],\n",
       "  ['deserve', 'die', 'motherfucke', 'learne', 'the', 'dope', 'like', 'she'],\n",
       "  ['a', 'nigga'],\n",
       "  ['to']],\n",
       " 54: [[\"'bou\", 'bu', 'fu', 'you'],\n",
       "  ['deserve', 'motherfucke', 'mone', 'the', 'me', 'die'],\n",
       "  ['to']],\n",
       " 55: [['livi', 'talki', 'i', 'fucki', 'wi', 'getti'],\n",
       "  [\"'bou\", 'bu', 'you', 'fu'],\n",
       "  ['me', 'mone']],\n",
       " 56: [['a', 'nigga'],\n",
       "  ['bu', 'you'],\n",
       "  ['wi', 'i', 'livi', 'fucki'],\n",
       "  ['made', 'me', 'millionaire', 'se']],\n",
       " 57: [['i', 'ai', 'livi'],\n",
       "  ['a', 'outta'],\n",
       "  ['made', 'the', 'millionaire', 'se']],\n",
       " 58: [['thu', 'you', 'cou'],\n",
       "  ['i', 'ai', 'livi'],\n",
       "  ['to', 'o', 'pisto', 'priso'],\n",
       "  ['slee', 'use', 'biggie', 'le', 'the', 'whe', 'remembe']],\n",
       " 59: [['slee', 'use', 'biggie', 'le', 'house', 'the', 'be', 'whe', 'remembe'],\n",
       "  ['i', 'bi'],\n",
       "  ['o', 'to'],\n",
       "  ['you', 'cou']],\n",
       " 60: [['a'],\n",
       "  ['slee', 'copie', 'le', 'versace', 'house', 'the', 'be', 'style'],\n",
       "  ['i', 'bi'],\n",
       "  ['no', 'to'],\n",
       "  ['you', 'abou']],\n",
       " 61: [['dro', 'no', 'too', 'sho'],\n",
       "  ['i'],\n",
       "  ['a'],\n",
       "  ['you', 'cou', 'abou'],\n",
       "  ['copie', 'smile', 'versace', 'five', 'me', 'style']],\n",
       " 62: [['se', 'smile', 'five', 'the', 'me'],\n",
       "  ['reco', 'sho', 'dro', 'no', 'too', 'to'],\n",
       "  [\"'bou\", 'cou'],\n",
       "  ['strai', 'i']],\n",
       " 63: [['no', 'reco', 'to'],\n",
       "  ['strai', 'i', 'wi', 'sti'],\n",
       "  [\"'bou\", 'thu', 'you'],\n",
       "  ['hate', 'the', 'se', 'love']],\n",
       " 64: [['wi', 'i', 'hi', 'sti'],\n",
       "  ['tha', 'a'],\n",
       "  [\"'e\", 'motherfucke', 'love', 'the', 'hate'],\n",
       "  ['thu', 'you', 'u']],\n",
       " 65: [['je', \"'e\", 'motherfucke', 'e'], ['i', 'hi']],\n",
       " 66: [['murde', 'where', 'je', 'ple', 'e']],\n",
       " 67: [['murde', 'where', 'he', 'we', 'ple'], ['you', 'occu']],\n",
       " 68: [['o', 'go', 'scenario', 'no', 'to'],\n",
       "  ['bri', 'poi'],\n",
       "  ['a', 'comma', 'drama'],\n",
       "  ['che', 'the', 'he', 'we']],\n",
       " 69: [['go', 'no', 'to', 'scenario'],\n",
       "  ['che', 'little', 'the', 'knee', 'fake']],\n",
       " 70: [['little', 'knee', 'fake', 'de'],\n",
       "  ['plea', 'cea'],\n",
       "  ['i', 'coppi', 'bri'],\n",
       "  ['you'],\n",
       "  ['janeiro', 'to']],\n",
       " 71: [['coppi', 'ki', 'i'],\n",
       "  ['coke', 'little', 'dope', 'de'],\n",
       "  ['janeiro', 'o']],\n",
       " 72: [['ge', 'little', 'smoke', 'dope', 'clique', 'whoppe', 'coke'],\n",
       "  ['i', 'ki'],\n",
       "  ['you', 'u'],\n",
       "  ['junio', 'o']],\n",
       " 73: [['ge', 'smoke', 'little', 'the', 'clique', 'whoppe'],\n",
       "  ['you', 'u', 'fu']],\n",
       " 74: [['a', 'cra', 'ma', 'wha'],\n",
       "  ['the', 'take', 'mone'],\n",
       "  ['fu', 'throu', 'you'],\n",
       "  ['stupi', 'i']],\n",
       " 75: [['looti', 'i', 'shooti', 'polluti', 'wi'],\n",
       "  ['clique', 'take', 'mone'],\n",
       "  ['ma', 'cra', 'a'],\n",
       "  ['throu', 'you'],\n",
       "  ['broo', 'blo']],\n",
       " 76: [['wi', 'looti', 'shooti', 'polluti'],\n",
       "  ['clique', 'cocke'],\n",
       "  ['a'],\n",
       "  ['you'],\n",
       "  ['sho', 'glo', 'blo', 'kno', 'to']],\n",
       " 77: [['wi', 'movi'],\n",
       "  ['mafia', 'a', 'outla'],\n",
       "  ['sho', 'glo', 'no', 'kno', 'to'],\n",
       "  ['anothe', 'clique', 'cocke'],\n",
       "  ['you', 'u']],\n",
       " 78: [['mafia', 'a', 'sta', 'outla'],\n",
       "  ['ge', 'droppe', 'moppe', 'clique', 'poppe', 'anothe'],\n",
       "  ['you', 'u'],\n",
       "  ['po', 'no']],\n",
       " 79: [['coa', 'sta', 'a', 'ea'],\n",
       "  ['you'],\n",
       "  ['po', 'pro'],\n",
       "  ['ge', 'droppe', 'moppe', 'poppe', 'fake']],\n",
       " 80: [['coa', 'a', 'ea'], ['locke', 'brainstorme', 'fake']],\n",
       " 81: [['locke', 'brainstorme', 'bite'], ['bea', 'a']],\n",
       " 82: [['bea', 'a', 'pa'], ['take', 'style', 'bite']],\n",
       " 83: [['a', 'pa'], ['te', 'face', 'take', 'fake', 'style']],\n",
       " 84: [['ali', 'shi', 'i', 'ai', 'wi'],\n",
       "  ['te', 'face', 'softe', 'chase', 'fake'],\n",
       "  ['bu', 'you'],\n",
       "  ['tha', 'a']],\n",
       " 85: [['murdere', 'ge', 'pape', 'the', 'softe', 'chase'],\n",
       "  ['tha', 'a'],\n",
       "  ['ali', 'wi']],\n",
       " 86: [['o', 'to', 'fo'],\n",
       "  ['murdere', 'ge', 'pape', 'the', 'scene', 'e', 'cape']],\n",
       " 87: [['little', 'the', 'choke', 'scene', 'e', 'cape', 'like'],\n",
       "  ['wi', 'i'],\n",
       "  ['mea', 'a', 'cea', 'approa'],\n",
       "  ['o', 'lo']],\n",
       " 88: [['smoke', 'little', 'we', 'joke', 'choke', 'like'],\n",
       "  ['cea', 'a'],\n",
       "  ['no', 'lo'],\n",
       "  ['motherfucki', 'i', 'toti', 'ai', 'wi']],\n",
       " 89: [['thu', 'gu'],\n",
       "  ['toti', 'ai', 'motherfucki'],\n",
       "  ['life', 'smoke', 'joke', 'we', 'be', 'bette'],\n",
       "  ['kno', 'no']],\n",
       " 90: [['thu', 'gu'], ['life', 'wide', 'we', 'ope', 'the', 'be', 'bette']],\n",
       " 91: [['wide', 'we', 'ope', 'the', 'battle', 'nee'],\n",
       "  ['i', 'smoki', 'hopi', 'approachi']],\n",
       " 92: [['o', 'lo', 'soo', 'go', 'no', 'fo'],\n",
       "  [\"'e\", 'nee', 'the', 'battle', 'crosse'],\n",
       "  ['i', 'hopi', 'boppi'],\n",
       "  ['a']],\n",
       " 93: [['i', 'hi', 'boppi'],\n",
       "  ['soo', 'go', 'o'],\n",
       "  ['the', 'crosse', \"'e\"],\n",
       "  ['a', 'nigga'],\n",
       "  ['fu', 'u']],\n",
       " 94: [['i', 'hi'], ['me', 'te', \"'e\"], ['you', 'u']],\n",
       " 95: [['wo', 'who', 'no'], ['ru', 'you'], ['see', 'me', 'te', 'the']],\n",
       " 96: [['see', 'the', 'take', 'mone'], ['ru', 'u'], ['hahaha', 'wanna']],\n",
       " 97: [['see', 'mone', 'whole', 'the', 'clique', 'take'],\n",
       "  ['junio', 'do'],\n",
       "  ['a', 'wanna']],\n",
       " 98: [['mone', 'whole', 'be', 'clique', 'take'],\n",
       "  ['junio', 'to'],\n",
       "  ['a'],\n",
       "  ['tryi', 'i', 'dressi']],\n",
       " 99: [['tryi', 'dressi'],\n",
       "  ['ou', 'fu', 'u'],\n",
       "  ['mo', 'o', 'ho', 'go', 'jo', 'to'],\n",
       "  ['we', 'mone', 'the', 'be', 'take', 'whe']],\n",
       " 100: [['mo', 'o', 'ho', 'go', 'jo'],\n",
       "  ['we', 'mone', 'the', 'be', 'millionaire', 'take', 'whe'],\n",
       "  ['ou', 'fu']],\n",
       " 101: [['mone', 'millionaire', 'we', 'take']],\n",
       " 102: [['killi', 'fai', 'i', 'ai', 'wi'],\n",
       "  ['bu', 'u', 'fu', 'you'],\n",
       "  ['mo', 'o', 'somebo', 'do'],\n",
       "  ['yea', 'gotta', 'wanna'],\n",
       "  ['dee', 'take', 'mone']],\n",
       " 103: [['mo', 'o'],\n",
       "  ['yea', 'a', 'wanna'],\n",
       "  ['motherfucke', 'little', 'dee', 'mone', 'take'],\n",
       "  ['you', 'u', 'fu']],\n",
       " 104: [['you'],\n",
       "  ['ce', 'motherfucke', 'sickle', 'little', 'one', 'mone', 'take'],\n",
       "  ['a', 'nigga']],\n",
       " 105: [['go', 'o', 'do'],\n",
       "  ['ce', 'sickle', 'one', 'mone', 'take', 'me'],\n",
       "  ['you'],\n",
       "  ['wi', 'somethi', 'fucki']],\n",
       " 106: [['arou', 'you', 'fu'],\n",
       "  ['wi', 'fucki'],\n",
       "  ['have', 'mone', 'seizure', 'take', 'me']],\n",
       " 107: [['a', 'hea', 'atta', 'ba', 'nigga'],\n",
       "  ['arou', 'you', 'u', 'fu'],\n",
       "  ['have',\n",
       "   'ge',\n",
       "   \"'fore\",\n",
       "   'mone',\n",
       "   'the',\n",
       "   'bette',\n",
       "   'take',\n",
       "   'smacke',\n",
       "   'seizure']],\n",
       " 108: [['ou', 'you', 'u', 'fu'],\n",
       "  ['ge', \"'fore\", 'we', 'the', 'bette', 'smacke', 'side']],\n",
       " 109: [['bri', 'i', 'thi'],\n",
       "  ['o', 'ho', 'fro', 'do', 'yo'],\n",
       "  ['side', 'ne', 'we'],\n",
       "  ['ou', 'you']],\n",
       " 110: [['a', 'wanna', 'drama', 'tha', 'nigga'],\n",
       "  ['yo', 'o', 'fro'],\n",
       "  ['bu', 'you'],\n",
       "  ['ne', 'we'],\n",
       "  ['singi', 'bringi', 'bri', 'i', 'ai']],\n",
       " 111: [['bu', 'fu', 'you'],\n",
       "  ['we'],\n",
       "  ['singi', 'ai', 'motherfucki', 'bringi'],\n",
       "  ['mama', 'a', 'drama']],\n",
       " 112: [['fu', 'you'], ['mama', 'a'], ['motherfucki', 'ki']],\n",
       " 113: [['biggie', 'motherfucke', 'we', 'whe', 'came'],\n",
       "  ['go', 'no', 'to'],\n",
       "  ['i', 'ki'],\n",
       "  ['wa', 'a'],\n",
       "  ['ju', 'ou', 'you', 'abou']],\n",
       " 114: [['opinio', 'no', 'everybo', 'to'],\n",
       "  ['biggie', 'ope', 'the', 'whe', 'came'],\n",
       "  ['wi', 'i', 'thei', 'motherfucki'],\n",
       "  ['ju', 'you', 'mou', 'ou', 'abou'],\n",
       "  ['wa', 'a', 'ha']],\n",
       " 115: [['the', 'we', 'ope'],\n",
       "  ['ho', 'go', 'everybo', 'do', 'opinio', 'to'],\n",
       "  ['a', 'ha'],\n",
       "  ['thei', 'motherfucki', 'thi', 'i', 'wi']],\n",
       " 116: [['dee', 'biggie', 'we'], ['i', 'thi'], ['mo', 'go', 'ho', 'do']],\n",
       " 117: [['fu'], ['mo', 'reco', 'bo'], ['cre', 'dee', 'biggie', 'labe']],\n",
       " 118: [['fu', 'you'],\n",
       "  ['ba', 'sta', 'wa', 'a'],\n",
       "  ['bo', 'reco', 'too', 'do', 'to'],\n",
       "  ['cre', 'the', 'be', 'labe'],\n",
       "  ['wi', 'i', 'motherfucki']],\n",
       " 119: [['ba', 'wa', 'a'],\n",
       "  ['wi', 'i'],\n",
       "  ['you', 'fu'],\n",
       "  ['bo', 'too', 'chino', 'do', 'to'],\n",
       "  ['the', 'be']],\n",
       " 120: [['too', 'chino'], ['fu', 'you']],\n",
       " 121: [['a', \"y'a\"],\n",
       "  ['you', 'fu'],\n",
       "  ['mone', 'die', 'motherfucke', 'take'],\n",
       "  ['slo', 'too', 'o']],\n",
       " 122: [['a', \"y'a\"],\n",
       "  ['slo', 'o', 'gro', 'do'],\n",
       "  ['sure', 'die', 'motherfucke', 'make'],\n",
       "  ['fu', 'you']],\n",
       " 123: [['see', 'motherfucke', 'make', 'be', 'sure'],\n",
       "  ['ca', 'a', \"y'a\"],\n",
       "  ['o', 'gro', 'do']],\n",
       " 124: [['thu', 'you', 'u'],\n",
       "  ['see', 'westside', 'motherfucke', 'life', 'we', 'ride', 'be', 'die']],\n",
       " 125: [['westside', 'warne', 'life', 'we', 'ride', 'here', 'die'],\n",
       "  ['i', \"'ti\", 'motherfucki'],\n",
       "  ['thu', 'ou']],\n",
       " 126: [['ou', 'you'],\n",
       "  ['warne', 'motherfucke', 'we', 'here'],\n",
       "  ['california', 'ya', 'nigga']],\n",
       " 127: [['the', 'we', 'motherfucke'],\n",
       "  ['mo', 'o', 'bo', 'do', 'jo'],\n",
       "  ['ou', 'you']],\n",
       " 128: [['bu', 'you'],\n",
       "  ['ai', 'motherfucki', 'nothi', 'thi'],\n",
       "  ['mo'],\n",
       "  ['rea', 'a', 'nigga'],\n",
       "  ['the', 'kille', 'we']],\n",
       " 129: [['ai', 'nothi'],\n",
       "  ['bu', 'u', 'you'],\n",
       "  ['the', 'fee', 'kille', 'motherfucke'],\n",
       "  ['rea', 'a', 'nigga']],\n",
       " 130: [['a'],\n",
       "  ['ou', 'you', 'u'],\n",
       "  ['quadruple', 'triple', 'motherfucke', 'fee']],\n",
       " 131: [['quadruple', 'triple', 'take', 'mone']],\n",
       " 132: [[\"'cause\", 'take', 'mone']],\n",
       " 133: [['ou', 'lau', 'you', 'gu'],\n",
       "  ['sta', 'nigga'],\n",
       "  [\"'cause\", 'the', 'be', 'motherfucke']],\n",
       " 134: [['you', 'gu'], ['i'], ['motherfucke', 'fe', 'we', 'the', 'be', 'whe']],\n",
       " 135: [['you'],\n",
       "  ['kno', 'reco', 'ho', 'dro'],\n",
       "  ['i'],\n",
       "  ['fee', 'fe', 'we', 'the', 'reale', 'whe']],\n",
       " 136: [['you', 'fu'],\n",
       "  ['ca', 'ba', 'nigga'],\n",
       "  [\"'e\", 'fee', 'we', 'the', 'reale', 'kille']]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assonances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
